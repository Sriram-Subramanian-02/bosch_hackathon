{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantic Chunking and RAG Fusion\n",
        "\n",
        "- This Notebook does semantic chunking of csv files otained from notebooks/summarize_image.ipynb, it creates chunks in such a way that the related textual content will be present in the same chunk, which are inserted into QDrant a collecction \"owners_manual_chunks\". \n",
        "\n",
        "- RAG Fusion is a process where, multiple queries will be created similar to the users query with the help of a LLM. With the help of reciprocal rank fusion, the documents are ranked and provided to the chat completion LLM as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6Ui17RdZF-Jd"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain\n",
        "!pip install langchain_openai langchain_experimental\n",
        "!pip install langchainhub\n",
        "!pip install langchain_together\n",
        "!pip -q install --upgrade together\n",
        "!pip install fastembed\n",
        "!pip install qdrant-client\n",
        "!pip install sentence_transformers\n",
        "!pip install langchain-cohere\n",
        "!pip install einops\n",
        "!pip install PyPDF2\n",
        "!pip install pypdf\n",
        "!pip install -U langchain-qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ZEelWy8VH1l3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"TOGETHER_API_KEY\"] = \"148521c4088ad416dced465cc144671626b00c860af4e6ebc855953567087d8a\"\n",
        "os.environ[\"COHERE_API_KEY\"] = 'xxe3X6u8vcTFJgJ8Pc7CfLezwpQiATQcUB56VIUp'\n",
        "\n",
        "TOP_K = 5\n",
        "MAX_DOCS_FOR_CONTEXT = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "xnd9ZYqnc2MB"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "from langchain.vectorstores import Qdrant\n",
        "from qdrant_client import QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client.http import models\n",
        "from langchain_core.documents.base import Document\n",
        "from langchain.load import dumps, loads\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "from langchain_core.retrievers import BaseRetriever\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
        "from langchain_cohere.llms import Cohere\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from sentence_transformers.util import cos_sim\n",
        "import PyPDF2\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain.embeddings.cohere import CohereEmbeddings\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_qdrant import Qdrant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TbUEh-0Azxi"
      },
      "outputs": [],
      "source": [
        "embedding_model = CohereEmbeddings(model = \"embed-english-v3.0\")\n",
        "semantic_chunker_embed_model = FastEmbedEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "V99hGlj2cnkA"
      },
      "outputs": [],
      "source": [
        "qdrant_client = QdrantClient(\n",
        "    \"https://8803fa99-7551-4f88-84c3-e134c9bed5de.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    prefer_grpc=True,\n",
        "    api_key=\"EFeN_UhdmAlDNYZHqJBUbZ88Nt7N0MkmvWLgM5Hs4ogNvExLMwNwdQ\",\n",
        ")\n",
        "\n",
        "QDRANT_URL = \"https://8803fa99-7551-4f88-84c3-e134c9bed5de.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
        "QDRANT_API_KEY = \"EFeN_UhdmAlDNYZHqJBUbZ88Nt7N0MkmvWLgM5Hs4ogNvExLMwNwdQ\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAbk73KXdO-i"
      },
      "outputs": [],
      "source": [
        "def create_QDrant_collection():\n",
        "\t\tembeddings = CohereEmbeddings(model = \"embed-english-v3.0\")\n",
        "\t\tqdrant_client.recreate_collection(\n",
        "\t\tcollection_name=\"owners_manual\",\n",
        "\t\tvectors_config = models.VectorParams(size=1024, distance=models.Distance.COSINE),\n",
        "\t)\n",
        "\n",
        "create_QDrant_collection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pj2OjJQ_fbnq"
      },
      "outputs": [],
      "source": [
        "def upload_chunks_to_QDrant(documents):\n",
        "  records_to_upload = []\n",
        "  for idx, chunk in enumerate(documents):\n",
        "      content = chunk.page_content\n",
        "      vector = embedding_model.encode(content).tolist()\n",
        "\n",
        "      record = models.PointStruct(\n",
        "          id=idx,\n",
        "          vector=vector,\n",
        "          payload={\"page_content\": content}\n",
        "      )\n",
        "      records_to_upload.append(record)\n",
        "\n",
        "  qdrant_client.upload_points(\n",
        "      collection_name=\"owners_manual\",\n",
        "      points=records_to_upload\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "PucnNRleX7Kf"
      },
      "outputs": [],
      "source": [
        "def upload_pdf_to_QDrant(pdf_path):\n",
        "    semantic_chunker = SemanticChunker(\n",
        "        semantic_chunker_embed_model, breakpoint_threshold_type=\"percentile\"\n",
        "    )\n",
        "\n",
        "    pdf_file_path = pdf_path\n",
        "    pdf_file = open(pdf_file_path, 'rb')\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "    text = ''\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        text = page.extract_text()\n",
        "        documents = semantic_chunker.create_documents([text])\n",
        "        upload_chunks_to_QDrant(documents)\n",
        "\n",
        "    pdf_file.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPLFoiCHYTJo"
      },
      "outputs": [],
      "source": [
        "upload_pdf_to_QDrant(\"/content/text.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "fDW1eZcYsuz1"
      },
      "outputs": [],
      "source": [
        "loader = PyPDFLoader(\"/content/text.pdf\")\n",
        "pages = loader.load_and_split()\n",
        "\n",
        "qdrant = Qdrant.from_documents(\n",
        "    pages,\n",
        "    embedding_model,\n",
        "    url=QDRANT_URL,\n",
        "    prefer_grpc=True,\n",
        "    api_key=QDRANT_API_KEY,\n",
        "    collection_name=\"owners_manual\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "syOqunU1N1fl"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"Please answer the [question] using only the following [information]. If there is no [information] available to answer the question, do not force an answer.\n",
        "\n",
        "Information: {context}\n",
        "\n",
        "Question: {question}\n",
        "Final answer:\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "RMDsb0oNHf2D"
      },
      "outputs": [],
      "source": [
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\"Rerank docs (Reciprocal Rank Fusion)\n",
        "\n",
        "    Args:\n",
        "        results (list[list]): retrieved documents\n",
        "        k (int, optional): parameter k for RRF. Defaults to 60.\n",
        "\n",
        "    Returns:\n",
        "        ranked_results: list of documents reranked by RRF\n",
        "    \"\"\"\n",
        "\n",
        "    fused_scores = {}\n",
        "    for docs in results:\n",
        "        # Assumes the docs are returned in sorted order of relevance\n",
        "        for rank, doc in enumerate(docs):\n",
        "            doc_str = dumps(doc)\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    ]\n",
        "\n",
        "    # return only documents\n",
        "    return [x[0] for x in reranked_results[:MAX_DOCS_FOR_CONTEXT]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "EMDHKjwZITwC"
      },
      "outputs": [],
      "source": [
        "def query_generator(original_query: dict) -> list[str]:\n",
        "    \"\"\"Generate queries from original query\n",
        "\n",
        "    Args:\n",
        "        query (dict): original query\n",
        "\n",
        "    Returns:\n",
        "        list[str]: list of generated queries\n",
        "    \"\"\"\n",
        "\n",
        "    # original query\n",
        "    query = original_query.get(\"query\")\n",
        "\n",
        "    # prompt for query generator\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful assistant that generates multiple search queries based on a single input query.\"),\n",
        "        (\"user\", \"Generate multiple search queries related to:  {original_query}. When creating queries, please refine or add closely related contextual information, without significantly altering the original query's meaning\"),\n",
        "        (\"user\", \"OUTPUT (3 queries):\")\n",
        "    ])\n",
        "\n",
        "    # LLM model\n",
        "    model = Cohere()\n",
        "\n",
        "    # query generator chain\n",
        "    query_generator_chain = (\n",
        "        prompt | model | StrOutputParser() | (lambda x: x.split(\"\\n\"))\n",
        "    )\n",
        "\n",
        "    # gererate queries\n",
        "    queries = query_generator_chain.invoke({\"original_query\": query})\n",
        "\n",
        "    # add original query\n",
        "    queries.insert(0, \"0. \" + query)\n",
        "\n",
        "    return queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4LGJuyflIc6j"
      },
      "outputs": [],
      "source": [
        "def rrf_retriever(query: str) -> list[Document]:\n",
        "    \"\"\"RRF retriever\n",
        "\n",
        "    Args:\n",
        "        query (str): Query string\n",
        "\n",
        "    Returns:\n",
        "        list[Document]: retrieved documents\n",
        "    \"\"\"\n",
        "\n",
        "    # Retriever\n",
        "    embedding = CohereEmbeddings(model = \"embed-english-v3.0\")\n",
        "\n",
        "\n",
        "    qdrant = Qdrant(\n",
        "        client=qdrant_client,\n",
        "        collection_name=\"owners_manual\",\n",
        "        embeddings=embedding,\n",
        "    )\n",
        "\n",
        "    retriever = qdrant.as_retriever()\n",
        "\n",
        "    # RRF chain\n",
        "    chain = (\n",
        "        {\"query\": itemgetter(\"query\")}\n",
        "        | RunnableLambda(query_generator)\n",
        "        | retriever.map()\n",
        "        | reciprocal_rank_fusion\n",
        "    )\n",
        "\n",
        "    # invoke\n",
        "    result = chain.invoke({\"query\": query})\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "mgQTzKAnIw80"
      },
      "outputs": [],
      "source": [
        "def query(query: str, retriever: BaseRetriever):\n",
        "    \"\"\"\n",
        "    Query with vectordb\n",
        "    \"\"\"\n",
        "\n",
        "    # model\n",
        "    model = Cohere()\n",
        "\n",
        "    # prompt\n",
        "    prompt = PromptTemplate(\n",
        "        template=template,\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "    )\n",
        "\n",
        "    # Query chain\n",
        "    chain = (\n",
        "        {\n",
        "            \"context\": itemgetter(\"question\") | retriever,\n",
        "            \"question\": itemgetter(\"question\")\n",
        "        }\n",
        "        | RunnablePassthrough.assign(\n",
        "            context=itemgetter(\"context\")\n",
        "        )\n",
        "        | {\n",
        "            \"response\": prompt | model | StrOutputParser(),\n",
        "            \"context\": itemgetter(\"context\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # execute chain\n",
        "    result = chain.invoke({\"question\": query})\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "ZPgHLHD7VcZo",
        "outputId": "13cac3e8-ea3a-4190-be3f-1d4e4ca75731"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hyundai provides a 3-year warranty from the date of sale, and the warranty is transferable to the subsequent owner for the remaining warranty period. The warranty policy offers comprehensive support to customers, including emergency roadside assistance, roadside repair, vehicle recovery, and jumping a dead battery. Customers are advised to visit a authorized Hyundai dealer workshop for assistance,  the full terms and conditions of which can be found online at the provided URL. \\n\\nIs there anything else you would like to know about Hyundai Warranty Policy? '"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "retriever = RunnableLambda(rrf_retriever)\n",
        "result = query(\"Explain Hyundai Warranty Policy\", retriever)\n",
        "result['response']"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
